{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549be034",
   "metadata": {},
   "source": [
    "## Neste código construiremos 3 redes neurais para aproximar derivadas.\n",
    "## Nos meus testes os resultados foram positivos, mas não posso garantir a consistência dos resultados devido a aleatoriedade do conjunto de dados criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12efab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "float_pres='float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23eb6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando conjunto de dados\n",
    "\n",
    "data_x_list=[]\n",
    "data_y_list=[]\n",
    "pi=np.pi\n",
    "\n",
    "\n",
    "Δx = 0.01                                 # Distância espacial dos pontos na malha utilizada\n",
    "x = tf.expand_dims(tf.range(-1, 1, Δx, dtype=float_pres),axis=0) # Gerando a malha de pontos no espaço unidimensional\n",
    "\n",
    "for i in range(5000):\n",
    "    \n",
    "    # Gerando uma condição inicial aleatória\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    k1 = tf.random.uniform([1], 0, 5, dtype='int32')   # Amostrando uma frequência aleatória para a função seno\n",
    "    k1 = tf.cast(k1, dtype=float_pres)                  # Mudando o tipo do tensor\n",
    "    a  = tf.random.uniform([1], 0, 1, dtype=float_pres) # Amostrando um peso aleatória para ponderar as funções seno\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Valor da função\n",
    "    u =     a * tf.math.sin(k1*pi*x)\n",
    "    # Valor da derivada\n",
    "    du= a*k1*pi*tf.math.cos(k1*pi*x)\n",
    "\n",
    "    \n",
    "    data_x_list.append(u)\n",
    "    data_y_list.append((u[:,:-1]-u[:,1:])/Δx)\n",
    "    \n",
    "    # Gerando uma condição inicial aleatória\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    k = tf.random.normal([1], 0, np.sqrt(20),dtype=float_pres)\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Valor da função\n",
    "    u = x*0 + k\n",
    "    # Valor da derivada\n",
    "    du= x*0\n",
    "\n",
    "    \n",
    "    data_x_list.append(u)\n",
    "    data_y_list.append((u[:,:-1]-u[:,1:])/Δx)\n",
    "    \n",
    "    # Gerando uma condição inicial aleatória\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    a = tf.random.normal([1], 0, np.sqrt(20),dtype=float_pres)\n",
    "    k = tf.random.normal([1], 0, np.sqrt(20),dtype=float_pres)\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Valor da função\n",
    "    u = a*x+k\n",
    "    # Valor da derivada\n",
    "    du= x*0+a\n",
    "\n",
    "    \n",
    "    data_x_list.append(u)\n",
    "    data_y_list.append((u[:,1:]-u[:,:-1])/Δx)\n",
    "    \n",
    "data_x=tf.concat(data_x_list,axis=0)\n",
    "data_y=tf.concat(data_y_list,axis=0)\n",
    "\n",
    "train_x=data_x[:int(15000*0.8)]\n",
    "train_y=data_y[:int(15000*0.8)]\n",
    "test_x=data_x[int(15000*0.8):]\n",
    "test_y=data_y[int(15000*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "857fc821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([12000, 199])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d09f7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Criando um modelo sequencial\n",
    "# model_foward=tf.keras.models.Sequential()\n",
    "\n",
    "# # Adicionando camadas\n",
    "# model_foward.add(tf.keras.layers.Reshape([400,1],dtype=float_pres)) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "#                                             # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "# model_foward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False,dtype=float_pres)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "#                                                                                                        # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "#                                                                                                        # Então o output da convolução será: n x (s-a+1) x b\n",
    "#                                                                                                        # Dada a natureza do problema, sabemos que não é necessário um bias (use_bias=False), então removemos ele para evitar overfitting.\n",
    "# model_foward.add(tf.keras.layers.Flatten(dtype=float_pres)) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "# model_foward.compile(loss='mean_squared_error',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea0d441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_y=tf.keras.layers.Input([200],dtype='float64')\n",
    "layers=[]\n",
    "layers.append(tf.keras.layers.Reshape([200,1],dtype=float_pres))\n",
    "layers.append(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False,dtype=float_pres))\n",
    "layers.append(tf.keras.layers.Flatten(dtype=float_pres))\n",
    "\n",
    "output_y=input_y/Δx\n",
    "\n",
    "for layer in layers:\n",
    "    output_y=layer(output_y)\n",
    "\n",
    "model_foward=tf.keras.Model(input_y, output_y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_foward.compile(loss='mean_squared_error',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2337880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "600/600 [==============================] - 4s 3ms/step - loss: 809.3066 - val_loss: 1084.9838\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 795.6268 - val_loss: 1084.4694\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 788.9760 - val_loss: 1084.0786\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 766.9370 - val_loss: 1083.5223\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 786.0712 - val_loss: 1083.1025\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 789.8687 - val_loss: 1082.3237\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 785.3183 - val_loss: 1081.7872\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 781.4075 - val_loss: 1081.5962\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 792.4211 - val_loss: 1081.0580\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 778.0292 - val_loss: 1080.9685\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 785.9345 - val_loss: 1080.8789\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 770.2584 - val_loss: 1080.5958\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 780.6470 - val_loss: 1080.4446\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 783.3596 - val_loss: 1080.5215\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 798.8826 - val_loss: 1080.1873\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 767.9940 - val_loss: 1079.9801\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 783.0492 - val_loss: 1079.9005\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 776.7219 - val_loss: 1079.7373\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 776.2371 - val_loss: 1079.7208\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 775.2281 - val_loss: 1079.6587\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 779.9474 - val_loss: 1079.5869\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 786.4444 - val_loss: 1079.4690\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 773.7253 - val_loss: 1079.3573\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 782.2301 - val_loss: 1079.3945\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 766.6651 - val_loss: 1079.2308\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 781.5112 - val_loss: 1079.1971\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 767.9324 - val_loss: 1079.1863\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 799.2360 - val_loss: 1079.1251\n",
      "Epoch 29/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 801.6128 - val_loss: 1079.0516\n",
      "Epoch 30/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 779.3643 - val_loss: 1079.1135\n",
      "Epoch 31/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 786.7460 - val_loss: 1079.1133\n",
      "Epoch 32/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 786.5992 - val_loss: 1079.1083\n",
      "Epoch 33/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 792.4227 - val_loss: 1079.1477\n",
      "Epoch 34/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 778.1481 - val_loss: 1079.0771\n",
      "Epoch 35/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 775.7503 - val_loss: 1079.1268\n",
      "Epoch 36/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 797.1332 - val_loss: 1079.0826\n",
      "Epoch 37/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 777.5623 - val_loss: 1079.0869\n",
      "Epoch 38/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 756.9252 - val_loss: 1078.9751\n",
      "Epoch 39/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 775.2189 - val_loss: 1078.9941\n",
      "Epoch 40/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 784.8918 - val_loss: 1078.9825\n",
      "Epoch 41/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 773.1290 - val_loss: 1078.9758\n",
      "Epoch 42/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 774.4330 - val_loss: 1078.9464\n",
      "Epoch 43/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 781.2537 - val_loss: 1078.8961\n",
      "Epoch 44/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 770.4013 - val_loss: 1078.9454\n",
      "Epoch 45/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 789.2830 - val_loss: 1078.9987\n",
      "Epoch 46/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 793.0187 - val_loss: 1078.9777\n",
      "Epoch 47/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 782.1743 - val_loss: 1078.9404\n",
      "Epoch 48/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 790.3515 - val_loss: 1078.9545\n",
      "Epoch 49/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 786.3772 - val_loss: 1079.0391\n",
      "Epoch 50/50\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 774.1263 - val_loss: 1079.1135\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1330 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1269 test_step\n        self.compiled_loss(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\losses.py:1204 mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10513 squared_difference\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3561 _create_op_internal\n        ret = Operation(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 199 and 198 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_DOUBLE](model_4/flatten_4/Reshape, IteratorGetNext:1)' with input shapes: [?,199], [?,198].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-5ee1a7e590d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                            \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                            validation_split=0.2)\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_foward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1501\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m-> 3038\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3457\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3459\u001b[1;33m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[0;32m   3460\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3379\u001b[0m           expand_composites=True)\n\u001b[0;32m   3380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3381\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3382\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1330 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1269 test_step\n        self.compiled_loss(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\losses.py:1204 mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10513 squared_difference\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3561 _create_op_internal\n        ret = Operation(\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 199 and 198 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_DOUBLE](model_4/flatten_4/Reshape, IteratorGetNext:1)' with input shapes: [?,199], [?,198].\n"
     ]
    }
   ],
   "source": [
    "history = model_foward.fit(train_x, train_y, # Como estamos calculando a derivada \"pra frente\", devemos remover a última derivada\n",
    "                           batch_size=16,\n",
    "                           epochs=50,\n",
    "                           validation_split=0.2)\n",
    "test_scores = model_foward.evaluate(test_x, test_y[:,:-1], verbose=1)\n",
    "print('Test loss:', test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8447f0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'conv1d_4/kernel:0' shape=(2, 1, 1) dtype=float64, numpy=\n",
       "array([[[ 0.89777252]],\n",
       "\n",
       "       [[-0.98328734]]])>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_foward.layers[3].weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "20de956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.015893298385611498, shape=(), dtype=float64)\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 117.7257\n",
      "117.72566223144531\n"
     ]
    }
   ],
   "source": [
    "# Coeficientes exatos\n",
    "print(tf.math.reduce_mean(((test_x[:,1:]-test_x[:,:-1])/Δx-test_y[:,:-1])**2))\n",
    "\n",
    "# Outros coeficientes\n",
    "print(model_foward.evaluate(test_x,test_y[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b45c275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_backward=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_backward.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "model_backward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "                                                                                                         # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                         # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_backward.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_backward.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_backward.fit(train_x, train_y[:,1:], # Como estamos calculando a derivada \"pra trás\", devemos remover a primeira derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_backward.evaluate(test_x, test_y[:,1:], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11250459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_backward.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5747959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_middle=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_middle.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "# Observe que, para a derivada centrada, o kernel_size deve ser 3.\n",
    "model_middle.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=3, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 398.\n",
    "                                                                                                       # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                       # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_middle.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_middle.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_middle.fit(train_x, train_y[:,1:-1], # Como estamos calculando a derivada centrada, devemos remover a primeira e a última derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_middle.evaluate(test_x, test_y[:,1:-1], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351413db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_middle.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81e05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
