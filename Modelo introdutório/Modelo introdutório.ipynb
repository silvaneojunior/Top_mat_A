{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a360da",
   "metadata": {},
   "source": [
    "## Neste código construiremos 3 redes neurais para aproximar derivadas.\n",
    "## Nos meus testes os resultados foram positivos, mas não posso garantir a consistência dos resultados devido a aleatoriedade do conjunto de dados criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11df3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "float_pres='float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f185a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando conjunto de dados\n",
    "\n",
    "data_x_list=[]\n",
    "data_y_list=[]\n",
    "pi=np.pi\n",
    "\n",
    "for i in range(50000):\n",
    "    Δx = 0.01                                 # Distância espacial dos pontos na malha utilizada\n",
    "    x = tf.range(-2, 2, Δx, dtype=float_pres) # Gerando a malha de pontos no espaço unidimensional\n",
    "    \n",
    "    # Gerando uma condição inicial aleatória\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    k1 = tf.random.uniform([1], 0, 20, dtype='int32')   # Amostrando uma frequência aleatória para a função seno\n",
    "    k1 = tf.cast(k1, dtype=float_pres)                  # Mudando o tipo do tensor\n",
    "    k2 = tf.random.uniform([1], 0, 20, dtype='int32')   # Amostrando uma frequência aleatória para a função seno\n",
    "    k2 = tf.cast(k2, dtype=float_pres)                  # Mudando o tipo do tensor\n",
    "    a  = tf.random.uniform([1], 0, 1, dtype=float_pres) # Amostrando um peso aleatória para ponderar as funções seno\n",
    "    b  = tf.random.uniform([1], 0, 2, dtype=float_pres) # Amostrando um modificador de amplitude aleatório\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Valor da função\n",
    "    u1 =     a * tf.expand_dims(tf.math.sin(k1*pi*x), axis=0) # Gerando pontos de acordo com a primeira função seno\n",
    "    u2 = (1-a) * tf.expand_dims(tf.math.sin(k2*pi*x), axis=0) # Gerando pontos de acordo com a segunda função seno\n",
    "    \n",
    "    # Valor da derivada\n",
    "    du1= a*k1*pi*tf.expand_dims(tf.math.cos(k1*pi*x), axis=0)\n",
    "    du2= (1-a)*k2*pi*tf.expand_dims(tf.math.cos(k2*pi*x), axis=0)\n",
    "    \n",
    "    u = b*(u1+u2) \n",
    "    du= b*(du1+du2)\n",
    "    \n",
    "    data_x_list.append(u)\n",
    "    data_y_list.append(du)\n",
    "\n",
    "data_x=tf.concat(data_x_list,axis=0)\n",
    "data_y=tf.concat(data_y_list,axis=0)\n",
    "\n",
    "train_x=data_x[:40000]\n",
    "train_y=data_y[:40000]\n",
    "test_x=data_x[-40000:]\n",
    "test_y=data_y[-40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3271af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_foward=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_foward.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "model_foward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False,dtype=float_pres)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "                                                                                                       # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                       # Então o output da convolução será: n x (s-a+1) x b\n",
    "                                                                                                       # Dada a natureza do problema, sabemos que não é necessário um bias (use_bias=False), então removemos ele para evitar overfitting.\n",
    "model_foward.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_foward.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_foward.fit(train_x, train_y[:,:-1], # Como estamos calculando a derivada \"pra frente\", devemos remover a última derivada\n",
    "                           batch_size=512,\n",
    "                           epochs=30,\n",
    "                           validation_split=0.2)\n",
    "test_scores = model_foward.evaluate(test_x, test_y[:,:-1], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_foward.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_backward=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_backward.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "model_backward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "                                                                                                         # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                         # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_backward.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_backward.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7be71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_backward.fit(train_x, train_y[:,1:], # Como estamos calculando a derivada \"pra trás\", devemos remover a primeira derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_backward.evaluate(test_x, test_y[:,1:], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c891f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_backward.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d159c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_middle=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_middle.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "# Observe que, para a derivada centrada, o kernel_size deve ser 3.\n",
    "model_middle.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=3, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 398.\n",
    "                                                                                                       # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                       # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_middle.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_middle.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_middle.fit(train_x, train_y[:,1:-1], # Como estamos calculando a derivada centrada, devemos remover a primeira e a última derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_middle.evaluate(test_x, test_y[:,1:-1], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187640a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_middle.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990d3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
