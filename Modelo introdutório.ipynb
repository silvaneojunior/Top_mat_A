{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b301f0bd",
   "metadata": {},
   "source": [
    "### Neste código construiremos 3 redes neurais para aproximar derivadas.\n",
    "### Nos meus testes os resultados foram positivos, mas não posso garantir a consistência dos resultados devido a aleatoriedade do conjunto de dados criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4da2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "float_pres='float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dab2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando conjunto de dados\n",
    "\n",
    "data_x_list=[]\n",
    "data_y_list=[]\n",
    "pi=np.pi\n",
    "\n",
    "for i in range(50000):\n",
    "    Δx = 0.01                                 # Distância espacial dos pontos na malha utilizada\n",
    "    x = tf.range(-2, 2, Δx, dtype=float_pres) # Gerando a malha de pontos no espaço unidimensional\n",
    "    \n",
    "    # Gerando uma condição inicial aleatória\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    k1 = tf.random.uniform([1], 0, 20, dtype='int32')   # Amostrando uma frequência aleatória para a função seno\n",
    "    k1 = tf.cast(k1, dtype=float_pres)                  # Mudando o tipo do tensor\n",
    "    k2 = tf.random.uniform([1], 0, 20, dtype='int32')   # Amostrando uma frequência aleatória para a função seno\n",
    "    k2 = tf.cast(k2, dtype=float_pres)                  # Mudando o tipo do tensor\n",
    "    a  = tf.random.uniform([1], 0, 1, dtype=float_pres) # Amostrando um peso aleatória para ponderar as funções seno\n",
    "    b  = tf.random.uniform([1], 0, 2, dtype=float_pres) # Amostrando um modificador de amplitude aleatório\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Valor da função\n",
    "    u1 =     a * tf.expand_dims(tf.math.sin(k1*pi*x), axis=0) # Gerando pontos de acordo com a primeira função seno\n",
    "    u2 = (1-a) * tf.expand_dims(tf.math.sin(k2*pi*x), axis=0) # Gerando pontos de acordo com a segunda função seno\n",
    "    \n",
    "    # Valor da derivada\n",
    "    du1= a*k1*pi*tf.expand_dims(tf.math.cos(k1*pi*x), axis=0)\n",
    "    du2= (1-a)*k2*pi*tf.expand_dims(tf.math.cos(k2*pi*x), axis=0)\n",
    "    \n",
    "    u = b*(u1+u2) \n",
    "    du= b*(du1+du2)\n",
    "    \n",
    "    data_x_list.append(u)\n",
    "    data_y_list.append(du)\n",
    "\n",
    "data_x=tf.concat(data_x_list,axis=0)\n",
    "data_y=tf.concat(data_y_list,axis=0)\n",
    "\n",
    "train_x=data_x[:40000]\n",
    "train_y=data_y[:40000]\n",
    "test_x=data_x[-10000:]\n",
    "test_y=data_y[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc7a9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_foward=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_foward.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "model_foward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False,dtype=float_pres)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "                                                                                                       # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                       # Então o output da convolução será: n x (s-a+1) x b\n",
    "                                                                                                       # Dada a natureza do problema, sabemos que não é necessário um bias (use_bias=False), então removemos ele para evitar overfitting.\n",
    "model_foward.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_foward.compile(loss='mean_squared_error',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d920354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 13.4183 - val_loss: 13.5986\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4224 - val_loss: 13.6053\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4188 - val_loss: 13.6001\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4279 - val_loss: 13.6134\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4234 - val_loss: 13.6011\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4238 - val_loss: 13.6114\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4198 - val_loss: 13.6005\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4207 - val_loss: 13.5986\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4198 - val_loss: 13.5986\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4195 - val_loss: 13.6128\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4212 - val_loss: 13.5989\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4190 - val_loss: 13.6191\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4277 - val_loss: 13.5988\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4275 - val_loss: 13.5986\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4226 - val_loss: 13.6085\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4232 - val_loss: 13.6065\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4232 - val_loss: 13.6132\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4193 - val_loss: 13.6020\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4154 - val_loss: 13.6119\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4353 - val_loss: 13.5985\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4252 - val_loss: 13.6019\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4218 - val_loss: 13.5992\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4182 - val_loss: 13.6415\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 13.4198 - val_loss: 13.6010\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4189 - val_loss: 13.6128\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 13.4279 - val_loss: 13.6010\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4253 - val_loss: 13.6001\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4252 - val_loss: 13.6110\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4201 - val_loss: 13.6000\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 13.4306 - val_loss: 13.6265\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 13.4733\n",
      "Test loss: 13.473310470581055\n"
     ]
    }
   ],
   "source": [
    "history = model_foward.fit(train_x, train_y[:,:-1], # Como estamos calculando a derivada \"pra frente\", devemos remover a última derivada\n",
    "                           batch_size=512,\n",
    "                           epochs=30,\n",
    "                           validation_split=0.2)\n",
    "test_scores = model_foward.evaluate(test_x, test_y[:,:-1], verbose=1)\n",
    "print('Test loss:', test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01d426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=float64, numpy=\n",
       "array([[[-0.95048655]],\n",
       "\n",
       "       [[ 1.01217661]]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_foward.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13a2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_backward=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_backward.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "model_backward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "                                                                                                         # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                         # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_backward.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_backward.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b10687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 514.9156 - mean_squared_error: 514.9156 - val_loss: 488.0776 - val_mean_squared_error: 488.0776\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 452.8593 - mean_squared_error: 452.8593 - val_loss: 427.2430 - val_mean_squared_error: 427.2430\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 394.6532 - mean_squared_error: 394.6532 - val_loss: 370.6004 - val_mean_squared_error: 370.6004\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 340.6747 - mean_squared_error: 340.6747 - val_loss: 318.1131 - val_mean_squared_error: 318.1131\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 290.8527 - mean_squared_error: 290.8527 - val_loss: 269.8255 - val_mean_squared_error: 269.8255\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 244.9574 - mean_squared_error: 244.9574 - val_loss: 225.6953 - val_mean_squared_error: 225.6953\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 203.3813 - mean_squared_error: 203.3813 - val_loss: 185.7198 - val_mean_squared_error: 185.7198\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 165.7408 - mean_squared_error: 165.7408 - val_loss: 149.9268 - val_mean_squared_error: 149.9268\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 132.6686 - mean_squared_error: 132.6686 - val_loss: 118.3060 - val_mean_squared_error: 118.3060\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 103.3862 - mean_squared_error: 103.3862 - val_loss: 90.8768 - val_mean_squared_error: 90.8768\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 78.1757 - mean_squared_error: 78.1757 - val_loss: 67.6293 - val_mean_squared_error: 67.6293\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 57.2123 - mean_squared_error: 57.2123 - val_loss: 48.5407 - val_mean_squared_error: 48.5407\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 40.4341 - mean_squared_error: 40.4341 - val_loss: 33.6430 - val_mean_squared_error: 33.6430\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 27.7194 - mean_squared_error: 27.7194 - val_loss: 22.9875 - val_mean_squared_error: 22.9875\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 19.2575 - mean_squared_error: 19.2575 - val_loss: 16.8371 - val_mean_squared_error: 16.8371\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 15.2443 - mean_squared_error: 15.2443 - val_loss: 14.5020 - val_mean_squared_error: 14.5020\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 13.9338 - mean_squared_error: 13.9338 - val_loss: 13.8434 - val_mean_squared_error: 13.8434\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.5584 - mean_squared_error: 13.5584 - val_loss: 13.6704 - val_mean_squared_error: 13.6704\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.4562 - mean_squared_error: 13.4562 - val_loss: 13.6362 - val_mean_squared_error: 13.6362\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.4557 - mean_squared_error: 13.4557 - val_loss: 13.6464 - val_mean_squared_error: 13.6464\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 13.4399 - mean_squared_error: 13.4399 - val_loss: 13.6070 - val_mean_squared_error: 13.6070\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 13.4211 - mean_squared_error: 13.4211 - val_loss: 13.6312 - val_mean_squared_error: 13.6312\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.4257 - mean_squared_error: 13.4257 - val_loss: 13.6007 - val_mean_squared_error: 13.6007\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.4287 - mean_squared_error: 13.4287 - val_loss: 13.6028 - val_mean_squared_error: 13.6028\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.4238 - mean_squared_error: 13.4238 - val_loss: 13.6410 - val_mean_squared_error: 13.6410\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.4328 - mean_squared_error: 13.4328 - val_loss: 13.6016 - val_mean_squared_error: 13.6016\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 13.4305 - mean_squared_error: 13.4305 - val_loss: 13.6007 - val_mean_squared_error: 13.6007\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 13.4228 - mean_squared_error: 13.4228 - val_loss: 13.6043 - val_mean_squared_error: 13.6043\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 13.4316 - mean_squared_error: 13.4316 - val_loss: 13.6004 - val_mean_squared_error: 13.6004\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 13.4275 - mean_squared_error: 13.4275 - val_loss: 13.6030 - val_mean_squared_error: 13.6030\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 13.4456 - mean_squared_error: 13.4456\n",
      "Test loss: 13.445581436157227\n",
      "Test accuracy: 13.445581436157227\n"
     ]
    }
   ],
   "source": [
    "history = model_backward.fit(train_x, train_y[:,1:], # Como estamos calculando a derivada \"pra trás\", devemos remover a primeira derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_backward.evaluate(test_x, test_y[:,1:], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c083e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=float32, numpy=\n",
       "array([[[-1.0128758 ]],\n",
       "\n",
       "       [[ 0.94938374]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_backward.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f8aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_middle=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_middle.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "# Observe que, para a derivada centrada, o kernel_size deve ser 3.\n",
    "model_middle.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=3, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 398.\n",
    "                                                                                                       # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                       # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_middle.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_middle.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05015dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 479.4188 - mean_squared_error: 479.4188 - val_loss: 422.3605 - val_mean_squared_error: 422.3605\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 363.6899 - mean_squared_error: 363.6899 - val_loss: 313.4422 - val_mean_squared_error: 313.4422\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 263.7209 - mean_squared_error: 263.7209 - val_loss: 220.7431 - val_mean_squared_error: 220.7431\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 180.0979 - mean_squared_error: 180.0979 - val_loss: 144.2630 - val_mean_squared_error: 144.2630\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 112.2075 - mean_squared_error: 112.2075 - val_loss: 84.0017 - val_mean_squared_error: 84.0017\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 60.5434 - mean_squared_error: 60.5434 - val_loss: 39.9592 - val_mean_squared_error: 39.9592\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 24.6909 - mean_squared_error: 24.6909 - val_loss: 12.1354 - val_mean_squared_error: 12.1354\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 5.0585 - mean_squared_error: 5.0585 - val_loss: 0.7076 - val_mean_squared_error: 0.7076\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2482 - mean_squared_error: 0.2482 - val_loss: 0.1542 - val_mean_squared_error: 0.1542\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1502 - mean_squared_error: 0.1502 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1500 - mean_squared_error: 0.1500 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.1535 - val_mean_squared_error: 0.1535\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.1507 - mean_squared_error: 0.1507\n",
      "Test loss: 0.15069149434566498\n",
      "Test accuracy: 0.15069149434566498\n"
     ]
    }
   ],
   "source": [
    "history = model_middle.fit(train_x, train_y[:,1:-1], # Como estamos calculando a derivada centrada, devemos remover a primeira e a última derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_middle.evaluate(test_x, test_y[:,1:-1], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b9d777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1), dtype=float32, numpy=\n",
       "array([[[-5.1925200e-01]],\n",
       "\n",
       "       [[ 4.1817068e-04]],\n",
       "\n",
       "       [[ 5.1882267e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -0.5, 0 e 0.5).\n",
    "model_middle.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f24aa6",
   "metadata": {},
   "source": [
    "### De maneira geral, os resultados foram satisfatórios, lembrando que o conjunto de dados é relativamente pequeno (50.000 observações), então é natural que a rede não consiga encontrar os pesos exatos devido a algum viés aleatório do conjunto de treino.\n",
    "### Fica a critério do usuário aumentar o tamanho do conjunto de treino afim de gerar aproximações melhores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263aac54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
