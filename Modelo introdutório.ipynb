{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0ad113",
   "metadata": {},
   "source": [
    "### Neste código construiremos 3 redes neurais para aproximar derivadas.\n",
    "### Nos meus testes os resultados foram positivos, mas não posso garantir a consistência dos resultados devido a aleatoriedade do conjunto de dados criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8b141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "float_pres='float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e59bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando conjunto de dados\n",
    "\n",
    "data_x_list=[]\n",
    "data_y_list=[]\n",
    "pi=np.pi\n",
    "\n",
    "for i in range(50000):\n",
    "    Δx = 0.01                                 # Distância espacial dos pontos na malha utilizada\n",
    "    x = tf.range(-2, 2, Δx, dtype=float_pres) # Gerando a malha de pontos no espaço unidimensional\n",
    "    \n",
    "    # Gerando uma condição inicial aleatória\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    k1 = tf.random.uniform([1], 0, 20, dtype='int32')   # Amostrando uma frequência aleatória para a função seno\n",
    "    k1 = tf.cast(k1, dtype=float_pres)                  # Mudando o tipo do tensor\n",
    "    k2 = tf.random.uniform([1], 0, 20, dtype='int32')   # Amostrando uma frequência aleatória para a função seno\n",
    "    k2 = tf.cast(k2, dtype=float_pres)                  # Mudando o tipo do tensor\n",
    "    a  = tf.random.uniform([1], 0, 1, dtype=float_pres) # Amostrando um peso aleatória para ponderar as funções seno\n",
    "    b  = tf.random.uniform([1], 0, 2, dtype=float_pres) # Amostrando um modificador de amplitude aleatório\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Valor da função\n",
    "    u1 =     a * tf.expand_dims(tf.math.sin(k1*pi*x), axis=0) # Gerando pontos de acordo com a primeira função seno\n",
    "    u2 = (1-a) * tf.expand_dims(tf.math.sin(k2*pi*x), axis=0) # Gerando pontos de acordo com a segunda função seno\n",
    "    \n",
    "    # Valor da derivada\n",
    "    du1= a*k1*pi*tf.expand_dims(tf.math.cos(k1*pi*x), axis=0)\n",
    "    du2= (1-a)*k2*pi*tf.expand_dims(tf.math.cos(k2*pi*x), axis=0)\n",
    "    \n",
    "    u = b*(u1+u2) \n",
    "    du= b*(du1+du2)\n",
    "    \n",
    "    data_x_list.append(u)\n",
    "    data_y_list.append(du)\n",
    "\n",
    "data_x=tf.concat(data_x_list,axis=0)\n",
    "data_y=tf.concat(data_y_list,axis=0)\n",
    "\n",
    "train_x=data_x[:40000]\n",
    "train_y=data_y[:40000]\n",
    "test_x=data_x[-40000:]\n",
    "test_y=data_y[-40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f016f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_foward=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_foward.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "model_foward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False,dtype=float_pres)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "                                                                                                       # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                       # Então o output da convolução será: n x (s-a+1) x b\n",
    "                                                                                                       # Dada a natureza do problema, sabemos que não é necessário um bias (use_bias=False), então removemos ele para evitar overfitting.\n",
    "model_foward.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_foward.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f6331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 3s 10ms/step - loss: 523.7717 - mean_squared_error: 523.7717 - val_loss: 492.3479 - val_mean_squared_error: 492.3479\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 460.4821 - mean_squared_error: 460.4821 - val_loss: 431.0152 - val_mean_squared_error: 431.0152\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 401.4950 - mean_squared_error: 401.4950 - val_loss: 373.8777 - val_mean_squared_error: 373.8777\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 346.4859 - mean_squared_error: 346.4859 - val_loss: 320.9509 - val_mean_squared_error: 320.9509\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 295.8019 - mean_squared_error: 295.8019 - val_loss: 272.2405 - val_mean_squared_error: 272.2405\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 249.5627 - mean_squared_error: 249.5627 - val_loss: 227.7016 - val_mean_squared_error: 227.7016\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 206.9529 - mean_squared_error: 206.9529 - val_loss: 187.3904 - val_mean_squared_error: 187.3904\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 168.8874 - mean_squared_error: 168.8874 - val_loss: 151.2756 - val_mean_squared_error: 151.2756\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 134.9534 - mean_squared_error: 134.9534 - val_loss: 119.3923 - val_mean_squared_error: 119.3923\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 105.2318 - mean_squared_error: 105.2318 - val_loss: 91.6734 - val_mean_squared_error: 91.6734\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 79.5799 - mean_squared_error: 79.5799 - val_loss: 68.1741 - val_mean_squared_error: 68.1741\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 58.2945 - mean_squared_error: 58.2945 - val_loss: 48.8848 - val_mean_squared_error: 48.8848\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 41.0701 - mean_squared_error: 41.0701 - val_loss: 33.8213 - val_mean_squared_error: 33.8213\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 28.1585 - mean_squared_error: 28.1585 - val_loss: 23.0254 - val_mean_squared_error: 23.0254\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 19.5415 - mean_squared_error: 19.5415 - val_loss: 16.6329 - val_mean_squared_error: 16.6329\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 15.3700 - mean_squared_error: 15.3700 - val_loss: 14.4146 - val_mean_squared_error: 14.4146\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 14.0868 - mean_squared_error: 14.0868 - val_loss: 13.7514 - val_mean_squared_error: 13.7514\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.7294 - mean_squared_error: 13.7294 - val_loss: 13.6304 - val_mean_squared_error: 13.6304\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6416 - mean_squared_error: 13.6416 - val_loss: 13.5300 - val_mean_squared_error: 13.5300\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6275 - mean_squared_error: 13.6275 - val_loss: 13.5162 - val_mean_squared_error: 13.5162\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6095 - mean_squared_error: 13.6095 - val_loss: 13.5330 - val_mean_squared_error: 13.5330\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6176 - mean_squared_error: 13.6176 - val_loss: 13.5110 - val_mean_squared_error: 13.5110\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6190 - mean_squared_error: 13.6190 - val_loss: 13.5236 - val_mean_squared_error: 13.5236\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6083 - mean_squared_error: 13.6083 - val_loss: 13.5809 - val_mean_squared_error: 13.5809\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6109 - mean_squared_error: 13.6109 - val_loss: 13.5122 - val_mean_squared_error: 13.5122\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6113 - mean_squared_error: 13.6113 - val_loss: 13.5135 - val_mean_squared_error: 13.5135\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6077 - mean_squared_error: 13.6077 - val_loss: 13.6095 - val_mean_squared_error: 13.6095\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6068 - mean_squared_error: 13.6068 - val_loss: 13.5046 - val_mean_squared_error: 13.5046\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6160 - mean_squared_error: 13.6160 - val_loss: 13.5110 - val_mean_squared_error: 13.5110\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6101 - mean_squared_error: 13.6101 - val_loss: 13.5230 - val_mean_squared_error: 13.5230\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 13.5810 - mean_squared_error: 13.5810\n",
      "Test loss: 13.580965995788574\n",
      "Test accuracy: 13.580965995788574\n"
     ]
    }
   ],
   "source": [
    "history = model_foward.fit(train_x, train_y[:,:-1], # Como estamos calculando a derivada \"pra frente\", devemos remover a última derivada\n",
    "                           batch_size=512,\n",
    "                           epochs=30,\n",
    "                           validation_split=0.2)\n",
    "test_scores = model_foward.evaluate(test_x, test_y[:,:-1], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15da9b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=float64, numpy=\n",
       "array([[[-0.95007384]],\n",
       "\n",
       "       [[ 1.01301466]]])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_foward.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7026879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_backward=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_backward.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "model_backward.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=2, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 399.\n",
    "                                                                                                         # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                         # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_backward.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_backward.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b06e2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 525.5174 - mean_squared_error: 525.5174 - val_loss: 494.0469 - val_mean_squared_error: 494.0469\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 462.0206 - mean_squared_error: 462.0206 - val_loss: 432.6075 - val_mean_squared_error: 432.6075\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 403.1559 - mean_squared_error: 403.1559 - val_loss: 375.3908 - val_mean_squared_error: 375.3908\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 348.0058 - mean_squared_error: 348.0058 - val_loss: 322.3504 - val_mean_squared_error: 322.3504\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 297.1942 - mean_squared_error: 297.1942 - val_loss: 273.5144 - val_mean_squared_error: 273.5144\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 250.6385 - mean_squared_error: 250.6385 - val_loss: 228.9058 - val_mean_squared_error: 228.9058\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 208.0852 - mean_squared_error: 208.0852 - val_loss: 188.4895 - val_mean_squared_error: 188.4895\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 169.7659 - mean_squared_error: 169.7659 - val_loss: 152.2464 - val_mean_squared_error: 152.2464\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 135.9353 - mean_squared_error: 135.9353 - val_loss: 120.2311 - val_mean_squared_error: 120.2311\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 105.8479 - mean_squared_error: 105.8479 - val_loss: 92.4275 - val_mean_squared_error: 92.4275\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 80.2733 - mean_squared_error: 80.2733 - val_loss: 68.8083 - val_mean_squared_error: 68.8083\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 58.8554 - mean_squared_error: 58.8554 - val_loss: 49.4234 - val_mean_squared_error: 49.4234\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 41.6623 - mean_squared_error: 41.6623 - val_loss: 34.2401 - val_mean_squared_error: 34.2401\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 28.4909 - mean_squared_error: 28.4909 - val_loss: 23.2382 - val_mean_squared_error: 23.2382\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 19.7489 - mean_squared_error: 19.7489 - val_loss: 16.7786 - val_mean_squared_error: 16.7786\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 15.4593 - mean_squared_error: 15.4593 - val_loss: 14.4076 - val_mean_squared_error: 14.4076\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 14.0978 - mean_squared_error: 14.0978 - val_loss: 13.7436 - val_mean_squared_error: 13.7436\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 13.7896 - mean_squared_error: 13.78 - 0s 6ms/step - loss: 13.7365 - mean_squared_error: 13.7365 - val_loss: 13.7073 - val_mean_squared_error: 13.7073\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6653 - mean_squared_error: 13.6653 - val_loss: 13.5251 - val_mean_squared_error: 13.5251\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6147 - mean_squared_error: 13.6147 - val_loss: 13.5094 - val_mean_squared_error: 13.5094\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6156 - mean_squared_error: 13.6156 - val_loss: 13.5190 - val_mean_squared_error: 13.5190\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 13.6124 - mean_squared_error: 13.6124 - val_loss: 13.5076 - val_mean_squared_error: 13.5076\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6124 - mean_squared_error: 13.6124 - val_loss: 13.5080 - val_mean_squared_error: 13.5080\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 13.6172 - mean_squared_error: 13.6172 - val_loss: 13.5076 - val_mean_squared_error: 13.5076\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 13.6230 - mean_squared_error: 13.6230 - val_loss: 13.5229 - val_mean_squared_error: 13.5229\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6041 - mean_squared_error: 13.6041 - val_loss: 13.5735 - val_mean_squared_error: 13.5735\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6167 - mean_squared_error: 13.6167 - val_loss: 13.5060 - val_mean_squared_error: 13.5060\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6189 - mean_squared_error: 13.6189 - val_loss: 13.5101 - val_mean_squared_error: 13.5101\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 13.6155 - mean_squared_error: 13.6155 - val_loss: 13.5132 - val_mean_squared_error: 13.5132\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 13.6222 - mean_squared_error: 13.6222 - val_loss: 13.5410 - val_mean_squared_error: 13.5410\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 13.5918 - mean_squared_error: 13.5918\n",
      "Test loss: 13.591769218444824\n",
      "Test accuracy: 13.591769218444824\n"
     ]
    }
   ],
   "source": [
    "history = model_backward.fit(train_x, train_y[:,1:], # Como estamos calculando a derivada \"pra trás\", devemos remover a primeira derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_backward.evaluate(test_x, test_y[:,1:], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a19053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=float32, numpy=\n",
       "array([[[-1.0124806 ]],\n",
       "\n",
       "       [[ 0.95030844]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -1 e 1).\n",
    "model_backward.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29bf9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo sequencial\n",
    "model_middle=tf.keras.models.Sequential()\n",
    "\n",
    "# Adicionando camadas\n",
    "model_middle.add(tf.keras.layers.Reshape([400,1])) # A camada de convolução exige uma dimensão extra para a \"cor\" da imagem,\n",
    "                                            # por isso o reshape para transformar o tensor de tamanho n x 400 em um tensor de tamanho n x 400 x 1\n",
    "# Observe que, para a derivada centrada, o kernel_size deve ser 3.\n",
    "model_middle.add(tf.keras.layers.Conv1D(filters=1,activation='linear', kernel_size=3, use_bias=False)) # Camada de convolução com ativação linear, após a convolução, a dimensão do vetor será n x 398.\n",
    "                                                                                                       # Lembrando que, se temos um input com dimensão n x s x r e uma convolução com filtro a x b (a é o tamanho do kernel e b é a quantidade de filtros) \n",
    "                                                                                                       # Então o output da convolução será: n x (s-a+1) x b\n",
    "model_middle.add(tf.keras.layers.Flatten()) # Esta camada remove a dimensão extra, transformando um tensor de tamanho n x 399 x 1 em um tensor de tamanho n x 399\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=10**-1, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "model_middle.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0bc095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 489.1279 - mean_squared_error: 489.1279 - val_loss: 427.3027 - val_mean_squared_error: 427.3027\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 370.9158 - mean_squared_error: 370.9158 - val_loss: 317.3097 - val_mean_squared_error: 317.3097\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 269.1168 - mean_squared_error: 269.1168 - val_loss: 223.6654 - val_mean_squared_error: 223.6654\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 184.0123 - mean_squared_error: 184.0123 - val_loss: 146.3653 - val_mean_squared_error: 146.3653\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 114.5868 - mean_squared_error: 114.5868 - val_loss: 85.4078 - val_mean_squared_error: 85.4078\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 61.8480 - mean_squared_error: 61.8480 - val_loss: 40.7924 - val_mean_squared_error: 40.7924\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 25.4107 - mean_squared_error: 25.4107 - val_loss: 12.5185 - val_mean_squared_error: 12.5185\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 5.2958 - mean_squared_error: 5.2958 - val_loss: 0.7492 - val_mean_squared_error: 0.7492\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2618 - mean_squared_error: 0.2618 - val_loss: 0.1530 - val_mean_squared_error: 0.1530\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1551 - mean_squared_error: 0.1551 - val_loss: 0.1525 - val_mean_squared_error: 0.1525\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1549 - mean_squared_error: 0.1549 - val_loss: 0.1519 - val_mean_squared_error: 0.1519\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1546 - mean_squared_error: 0.1546 - val_loss: 0.1518 - val_mean_squared_error: 0.1518\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1544 - mean_squared_error: 0.1544 - val_loss: 0.1516 - val_mean_squared_error: 0.1516\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1543 - mean_squared_error: 0.1543 - val_loss: 0.1514 - val_mean_squared_error: 0.1514\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1542 - mean_squared_error: 0.1542 - val_loss: 0.1513 - val_mean_squared_error: 0.1513\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1540 - mean_squared_error: 0.1540 - val_loss: 0.1512 - val_mean_squared_error: 0.1512\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1541 - mean_squared_error: 0.15 - 0s 5ms/step - loss: 0.1539 - mean_squared_error: 0.1539 - val_loss: 0.1510 - val_mean_squared_error: 0.1510\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1537 - mean_squared_error: 0.1537 - val_loss: 0.1509 - val_mean_squared_error: 0.1509\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1536 - mean_squared_error: 0.1536 - val_loss: 0.1508 - val_mean_squared_error: 0.1508\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1535 - mean_squared_error: 0.1535 - val_loss: 0.1507 - val_mean_squared_error: 0.1507\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1534 - mean_squared_error: 0.1534 - val_loss: 0.1507 - val_mean_squared_error: 0.1507\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1533 - mean_squared_error: 0.1533 - val_loss: 0.1506 - val_mean_squared_error: 0.1506\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1533 - mean_squared_error: 0.1533 - val_loss: 0.1505 - val_mean_squared_error: 0.1505\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1532 - mean_squared_error: 0.1532 - val_loss: 0.1505 - val_mean_squared_error: 0.1505\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1532 - mean_squared_error: 0.1532 - val_loss: 0.1504 - val_mean_squared_error: 0.1504\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1532 - mean_squared_error: 0.1532 - val_loss: 0.1504 - val_mean_squared_error: 0.1504\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1531 - mean_squared_error: 0.1531 - val_loss: 0.1504 - val_mean_squared_error: 0.1504\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1531 - mean_squared_error: 0.1531 - val_loss: 0.1504 - val_mean_squared_error: 0.1504\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1531 - mean_squared_error: 0.1531 - val_loss: 0.1503 - val_mean_squared_error: 0.1503\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1531 - mean_squared_error: 0.1531 - val_loss: 0.1503 - val_mean_squared_error: 0.1503\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.1525 - mean_squared_error: 0.1525\n",
      "Test loss: 0.15252622961997986\n",
      "Test accuracy: 0.15252622961997986\n"
     ]
    }
   ],
   "source": [
    "history = model_middle.fit(train_x, train_y[:,1:-1], # Como estamos calculando a derivada centrada, devemos remover a primeira e a última derivada\n",
    "                    batch_size=512,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model_middle.evaluate(test_x, test_y[:,1:-1], verbose=1)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8edeb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1), dtype=float32, numpy=\n",
       "array([[[-0.5182046 ]],\n",
       "\n",
       "       [[-0.00181072]],\n",
       "\n",
       "       [[ 0.5201305 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando os pesos da segunda camada (a de convolução) da rede e multiplicando por Δx para ver se o valor está correto (o ideal seria -0.5, 0 e 0.5).\n",
    "model_middle.layers[1].weights[0]*Δx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51150775",
   "metadata": {},
   "source": [
    "### De maneira geral, os resultados foram satisfatórios, lembrando que o conjunto de dados é relativamente pequeno (50.000 observações), então é natural que a rede não consiga encontrar os pesos exatos devido a algum viés aleatório do conjunto de treino.\n",
    "### Fica a critério do usuário aumentar o tamanho do conjunto de treino afim de gerar aproximações melhores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0876d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
